{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44fd3ce3",
   "metadata": {},
   "source": [
    "# üöÄ GRPO Training Tutorial: Teaching Models to Reason with MLX-LM-LoRA\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **Group Relative Policy Optimization (GRPO)** for training reasoning models on **Apple Silicon** using **MLX-LM-LoRA**. GRPO is a reinforcement learning technique that improves model outputs using reward signals from multiple reward functions.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **Two-stage training approach**: Cold-start SFT ‚Üí GRPO fine-tuning\n",
    "2. **Efficient training on Apple Silicon** with 8-bit quantization and LoRA\n",
    "3. **Custom reward functions** for structured reasoning outputs\n",
    "4. **Long context handling** (up to 4096 tokens)\n",
    "\n",
    "### Training Pipeline\n",
    "\n",
    "**Stage 1: Cold Start (SFT)** ‚Üí Teach the model the basic reasoning format  \n",
    "**Stage 2: GRPO** ‚Üí Optimize the model using reward-based feedback to improve reasoning quality\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Apple Silicon Mac (M1/M2/M3/M4)\n",
    "- MLX-LM-LoRA installed\n",
    "- Sufficient RAM (16GB+ recommended)\n",
    "\n",
    "Let's begin! üéØ\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Import Required Libraries\n",
    "\n",
    "First, we import all necessary libraries from MLX-LM-LoRA for training, generation, and utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa87f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm_lora.utils import save_pretrained_merged, from_pretrained, calculate_iters, push_to_hub\n",
    "from mlx_lm_lora.trainer.grpo_trainer import GRPOTrainingArgs, train_grpo, evaluate_grpo\n",
    "from mlx_lm_lora.trainer.sft_trainer import SFTTrainingArgs, train_sft, evaluate_sft\n",
    "from mlx_lm_lora.trainer.datasets import CacheDataset, GRPODataset, TextDataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from mlx_lm.tuner.utils import print_trainable_parameters\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback, WandBCallback\n",
    "from mlx_lm.generate import generate, make_sampler\n",
    "\n",
    "import mlx.optimizers as optim\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcfcd78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Configure Model and Training Parameters\n",
    "\n",
    "Here we define all configuration parameters for our training pipeline:\n",
    "\n",
    "### Model Configuration\n",
    "- **Base model**: A pre-trained instruction-tuned model\n",
    "- **Cold start model**: Intermediate model trained with supervised learning\n",
    "- **Final model**: GRPO-optimized model with improved reasoning\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "**Sequence Lengths:**\n",
    "- `cold_start_max_seq_length`: 2048 tokens (for initial SFT training)\n",
    "- `zero_max_seq_length`: 4096 tokens (for GRPO training with longer reasoning)\n",
    "\n",
    "**LoRA Configuration:**\n",
    "- `rank`: 8 (controls adapter size)\n",
    "- `scale`: 10.0 (LoRA scaling factor)\n",
    "- `num_layers`: 8 (number of layers to adapt)\n",
    "\n",
    "**Quantization:**\n",
    "- `bits`: 8 (8-bit quantization for memory efficiency)\n",
    "- `group_size`: 128 (quantization granularity)\n",
    "\n",
    "**Reasoning Format:**\n",
    "We use special tokens to structure the model's reasoning:\n",
    "- `<think>...</think>`: Contains the model's working/reasoning\n",
    "- `<answer>...</answer>`: Contains the final answer\n",
    "\n",
    "This format helps the model learn to separate reasoning from conclusions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Goekdeniz-Guelmez/Llama-3.2-1B-Instruct-gabliterated\"\n",
    "new_cold_start_model_name = \"Llama-3.2-1B-Gabliterated-Zero-Cold_Start\"\n",
    "new_zero_model_name = \"Llama-3.2-1B-Gabliterated-Zero\"\n",
    "user_name = \"Goekdeniz-Guelmez\"\n",
    "\n",
    "cold_start_max_seq_length = 2048\n",
    "zero_max_seq_length = 4096\n",
    "\n",
    "cold_start_adapter_path = f\"./{new_cold_start_model_name}\"\n",
    "zero_adapter_path = f\"./{new_zero_model_name}\"\n",
    "\n",
    "grpo_dataset_name = \"mlx-community/gsm8k\"\n",
    "lora_config = {\n",
    "    \"rank\": 8,\n",
    "    \"dropout\": 0.0,\n",
    "    \"scale\": 10.0,\n",
    "    \"use_dora\": False,\n",
    "    \"num_layers\": 8\n",
    "}\n",
    "quantized_load={\n",
    "    \"bits\": 8,\n",
    "    \"group_size\": 128\n",
    "}\n",
    "\n",
    "reasoning_start = \"<think>\"\n",
    "reasoning_end   = \"</think>\"\n",
    "solution_start = \"<answer>\"\n",
    "solution_end = \"</answer>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f1a92",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Load the Base Model for Cold Start Training\n",
    "\n",
    "### What is Cold Start Training?\n",
    "\n",
    "Before we can use GRPO (which requires the model to generate responses), we need to teach the model the **basic reasoning format**. This is called \"cold start\" training and uses standard **Supervised Fine-Tuning (SFT)**.\n",
    "\n",
    "### Why Cold Start?\n",
    "\n",
    "- **Problem**: The base model doesn't know how to use `<think>` and `<answer>` tags\n",
    "- **Solution**: First train it with examples that have the correct format\n",
    "- **Result**: Model learns the structure before we optimize with GRPO\n",
    "\n",
    "### Model Loading\n",
    "\n",
    "We load the model with:\n",
    "- **LoRA adapters**: Only ~1-2% of parameters are trainable (memory efficient!)\n",
    "- **8-bit quantization**: Reduces memory usage by ~50%\n",
    "- **New adapter path**: Creates a fresh adapter for cold start training\n",
    "\n",
    "The `print_trainable_parameters()` function shows how few parameters we're actually training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab79254",
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_start_model, cold_start_tokenizer, cold_start_adapter_file = from_pretrained(\n",
    "    model=model_name,\n",
    "    lora_config=lora_config,\n",
    "    quantized_load=quantized_load,\n",
    "    new_adapter_path=cold_start_adapter_path\n",
    ")\n",
    "print_trainable_parameters(cold_start_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf402ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Prepare the Cold Start Dataset\n",
    "\n",
    "### Dataset Formatting\n",
    "\n",
    "We need to format the dataset to teach the model our reasoning structure. Each example contains:\n",
    "- **System prompt**: Instructions on how to structure the response\n",
    "- **User message**: The problem to solve\n",
    "- **Assistant response**: The correctly formatted answer with reasoning\n",
    "\n",
    "### Format Structure\n",
    "\n",
    "```\n",
    "<think>\n",
    "[Step-by-step reasoning and working]\n",
    "</think>\n",
    "<answer>[Final answer]</answer>\n",
    "```\n",
    "\n",
    "### Dataset Source\n",
    "\n",
    "We use the **GSM8K dataset** (Grade School Math problems) which includes:\n",
    "- `prompt`: The math problem\n",
    "- `reasoning`: Step-by-step solution\n",
    "- `answer`: Final numerical answer\n",
    "\n",
    "### Why Take Only 1000 Examples?\n",
    "\n",
    "For cold start, we don't need the full dataset - just enough examples to teach the format. This:\n",
    "- Reduces training time significantly\n",
    "- Prevents overfitting to the cold start distribution\n",
    "- Leaves the model flexible for GRPO optimization\n",
    "\n",
    "The `TextDataset` wrapper prepares the data for efficient SFT training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41404f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = f\"You are given a problem. Think about the problem and provide your working out. Place it between {reasoning_start} and {reasoning_end}. Then, provide your solution between {solution_start} {solution_end}.\"\n",
    "\n",
    "def format_cold_start(sample):\n",
    "    raw_answer = f\"{reasoning_start}\\n{sample[\"reasoning\"]}\\n{reasoning_end}\\n{solution_start}{sample[\"answer\"]}{solution_end}\"\n",
    "\n",
    "    sample[\"text\"] = cold_start_tokenizer.apply_chat_template(\n",
    "        conversation=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": sample[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": raw_answer},\n",
    "        ],\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=False\n",
    "    )\n",
    "    return sample\n",
    "\n",
    "cold_start_train_dataset = load_dataset(grpo_dataset_name)[\"test\"].take(1000).map(format_cold_start, )\n",
    "\n",
    "cold_start_train_set = TextDataset(\n",
    "    cold_start_train_dataset,\n",
    "    tokenizer=cold_start_tokenizer,\n",
    "    text_key=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5066b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cold_start_train_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aaefe0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Train the Cold Start Model with Supervised Fine-Tuning\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "**Memory Optimization:**\n",
    "- `batch_size=1`: Minimal memory usage (increase if you have more RAM)\n",
    "- `gradient_accumulation_steps=32`: Simulates batch size of 32 by accumulating gradients\n",
    "- `grad_checkpoint=True`: Trades compute for memory (gradient checkpointing)\n",
    "\n",
    "**Training Duration:**\n",
    "- `epochs=1`: We only need one pass through the data to learn the format\n",
    "- `iters`: Automatically calculated based on dataset size\n",
    "\n",
    "**Monitoring:**\n",
    "- `steps_per_report=100`: Log training metrics every 100 steps\n",
    "- `steps_per_eval=200`: Run validation every 200 steps\n",
    "- `steps_per_save=400`: Save checkpoint every 400 steps\n",
    "\n",
    "### What Happens During Training?\n",
    "\n",
    "The model learns to:\n",
    "1. Recognize when to use reasoning tags\n",
    "2. Generate step-by-step working inside `<think>` tags\n",
    "3. Provide final answers inside `<answer>` tags\n",
    "\n",
    "### Training Callbacks\n",
    "\n",
    "You can use:\n",
    "- `TrainingCallback()`: Basic console logging (default)\n",
    "- `WandBCallback()`: Advanced logging with Weights & Biases (commented out)\n",
    "\n",
    "**Training will take 10-30 minutes depending on your Mac's performance.**\n",
    "\n",
    "After training, we save the merged model (base model + adapters combined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39072ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.AdamW(learning_rate=8e-5)\n",
    "\n",
    "# Training arguments. Adjust these based on your dataset size, GPU capacity, and how long you want to train.\n",
    "args = SFTTrainingArgs(\n",
    "    batch_size=1, # Use batch size of 1 to save RAM, increase if you have more GPU memory\n",
    "    iters=calculate_iters(cold_start_train_set, batch_size=1, epochs=1), # Only train for 1 epoch since the dataset is small, increase if you want to train longer\n",
    "    gradient_accumulation_steps=32, # Accumulate gradients over 8 steps to simulate a larger batch size and save RAM. Adjust based on your GPU capacity.\n",
    "    val_batches=1, # Only use 1 batch for validation to speed it up, since the dataset is small. Remove or increase for better evaluation.\n",
    "    steps_per_report=100, # Log training progress every 10 steps\n",
    "    steps_per_eval=200, # Evaluate every 20 steps\n",
    "    steps_per_save=400, # Save the model every 10 steps\n",
    "    max_seq_length=cold_start_max_seq_length,\n",
    "    adapter_file=cold_start_adapter_file,\n",
    "    grad_checkpoint=True, # Use gradient checkpointing to save RAM at the cost of slightly slower training\n",
    ")\n",
    "\n",
    "# Start training\n",
    "train_sft(\n",
    "    model=cold_start_model,\n",
    "    args=args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=CacheDataset(cold_start_train_set),\n",
    "    # training_callback=WandBCallback(\n",
    "    #     project_name=f\"{new_model_name}-finetuning\",\n",
    "    #     log_dir=adapter_path,\n",
    "    #     wrapped_callback=TrainingCallback(),\n",
    "    #     config=None\n",
    "    # )\n",
    "    training_callback=TrainingCallback(), # You can use the basic TrainingCallback to log training progress to the console instead of Weights & Biases. Just comment out the WandBCallback and uncomment this line if you prefer that.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de49277",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pretrained_merged(\n",
    "    model=cold_start_model,\n",
    "    tokenizer=cold_start_tokenizer,\n",
    "    save_path=cold_start_adapter_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b0154",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Transition to GRPO Training\n",
    "\n",
    "### Cleaning Up Memory\n",
    "\n",
    "First, we delete the cold start resources to free up memory. This is **crucial on Apple Silicon** to avoid running out of RAM during GRPO training.\n",
    "\n",
    "### Loading Models for GRPO\n",
    "\n",
    "We need **TWO models** for GRPO:\n",
    "\n",
    "1. **Policy Model** (model to optimize)\n",
    "   - Loaded from the cold start checkpoint\n",
    "   - Has fresh LoRA adapters to train\n",
    "   - Will be updated during GRPO\n",
    "\n",
    "2. **Reference Model** (baseline for KL divergence)\n",
    "   - Loaded from the **original base model** (not cold start!)\n",
    "   - Frozen (no training)\n",
    "   - Used to compute KL penalty to prevent the model from drifting too far\n",
    "\n",
    "### Why Use Base Model as Reference?\n",
    "\n",
    "Using the **original base model** (instead of cold start) as reference:\n",
    "- ‚úÖ Provides better KL divergence regularization\n",
    "- ‚úÖ Prevents the model from overfitting to GRPO rewards\n",
    "- ‚úÖ Maintains general language capabilities\n",
    "- ‚úÖ Results in more robust reasoning\n",
    "\n",
    "### Longer Context\n",
    "\n",
    "Note we now use `zero_max_seq_length=4096` (double the cold start length) because:\n",
    "- Reasoning chains can be much longer\n",
    "- GRPO generates completions for multiple candidates\n",
    "- We need space for detailed step-by-step thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c11b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del cold_start_model\n",
    "del cold_start_tokenizer\n",
    "del cold_start_adapter_file\n",
    "del cold_start_train_set\n",
    "del cold_start_train_dataset\n",
    "del opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53528362",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, adapter_file = from_pretrained(\n",
    "    model=cold_start_adapter_path,\n",
    "    lora_config=lora_config,\n",
    "    quantized_load=quantized_load,\n",
    "    new_adapter_path=zero_adapter_path\n",
    ")\n",
    "\n",
    "# Use the base model as reference instead of the cold-start model\n",
    "# This provides better KL divergence regularization during GRPO\n",
    "ref_model, _, _ = from_pretrained(\n",
    "    model=cold_start_adapter_path,\n",
    "    quantized_load=quantized_load,\n",
    ")\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ab15d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Define Custom Reward Functions\n",
    "\n",
    "### What are Reward Functions?\n",
    "\n",
    "GRPO optimizes the model by **rewarding good behaviors**. We define multiple reward functions to evaluate different aspects of the model's responses.\n",
    "\n",
    "### Our Four Reward Functions\n",
    "\n",
    "#### 1. `match_format_exactly` (Reward: +3.0)\n",
    "- Checks if the response **perfectly follows** the format\n",
    "- Must have reasoning ending with `</think>`, followed by answer in `<answer>...</answer>`\n",
    "- Strictest reward - only given for perfectly structured responses\n",
    "\n",
    "#### 2. `match_format_approximately` (Reward: ¬±0.5 per component)\n",
    "- More forgiving than exact matching\n",
    "- Checks for presence of key components:\n",
    "  - One `</think>` tag ‚Üí +0.5\n",
    "  - One `<answer>` tag ‚Üí +0.5  \n",
    "  - One `</answer>` tag ‚Üí +0.5\n",
    "  - No extra `<think>` tags ‚Üí 0 (penalty if multiple)\n",
    "- Helps guide the model even when format isn't perfect\n",
    "\n",
    "#### 3. `check_answer` (Reward: 0 to +3.0)\n",
    "- **Most important**: Checks if the answer is correct!\n",
    "- Exact match ‚Üí +3.0\n",
    "- Close match (with whitespace) ‚Üí +1.5\n",
    "- Numerical approximation (¬±10%) ‚Üí +0.5\n",
    "- Numerical approximation (¬±20%) ‚Üí +0.25\n",
    "- Wrong answer ‚Üí -0.5 or -1.0 penalty\n",
    "\n",
    "#### 4. `check_numbers` (Reward: 0 or +1.5)\n",
    "- Simpler numerical check\n",
    "- Extracts first number after `<answer>`\n",
    "- Exact match ‚Üí +1.5\n",
    "- No match or wrong ‚Üí 0\n",
    "\n",
    "### How Rewards Work Together\n",
    "\n",
    "The model receives signals about:\n",
    "- **Structure** (format rewards) ‚Üí Learn how to organize output\n",
    "- **Correctness** (answer rewards) ‚Üí Learn to solve problems accurately\n",
    "- **Consistency** (number rewards) ‚Üí Learn to be consistent\n",
    "\n",
    "These combined rewards guide the model to generate well-structured AND correct reasoning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eee7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_format = re.compile(\n",
    "    rf\".+?\\n{reasoning_end}\\n{solution_start}(.+?){solution_end}\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "def match_format_exactly(prompts, completions, answer, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def match_format_approximately(prompts, completions, answer, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
    "        score -= 0.5 if response.count(reasoning_start) >= 1 else 0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    responses = [completion[0][\"content\"] if isinstance(completion, list) else completion for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        if guess == true_answer:\n",
    "            score += 3.0\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 1.5\n",
    "        else:\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if   ratio >= 0.9 and ratio <= 1.1: score += 0.5\n",
    "                elif ratio >= 0.8 and ratio <= 1.2: score += 0.25\n",
    "                else: score -= 1.0\n",
    "            except:\n",
    "                score -= 0.5\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "match_numbers = re.compile(\n",
    "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    responses = [completion[0][\"content\"] if isinstance(completion, list) else completion for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            guess       = float(guess.strip())\n",
    "            scores.append(1.5 if guess == true_answer else 0.0)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0473a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Prepare the GRPO Training Dataset\n",
    "\n",
    "### GRPODataset vs TextDataset\n",
    "\n",
    "Unlike cold start training where we had pre-formatted text, GRPO needs a **different data format**:\n",
    "\n",
    "- **TextDataset** (cold start): Complete input-output pairs for supervised learning\n",
    "- **GRPODataset** (GRPO): Prompts that the model will complete and evaluate\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "The `GRPODataset` expects:\n",
    "- `prompt_key=\"prompt\"`: The problem statement\n",
    "- `answer_key=\"answer\"`: Ground truth for reward calculation\n",
    "- `system_key=\"system\"`: Optional system message\n",
    "- `type_key=\"type\"`: Optional task type identifier\n",
    "\n",
    "### Why Only 100 Examples?\n",
    "\n",
    "GRPO is **much more compute-intensive** than SFT because:\n",
    "- Each training step generates **multiple completions** (group_size=4)\n",
    "- Each completion can be very long (up to 2048 tokens)\n",
    "- All completions are evaluated by 4 reward functions\n",
    "- Everything runs on your Mac's unified memory\n",
    "\n",
    "100 examples √ó 4 completions √ó multiple epochs = substantial training!\n",
    "\n",
    "### Memory Considerations\n",
    "\n",
    "On Apple Silicon, we're constrained by unified memory. This smaller dataset:\n",
    "- Keeps training time reasonable (1-3 hours)\n",
    "- Prevents OOM (Out of Memory) errors\n",
    "- Still provides strong learning signal with GRPO's group-based optimization\n",
    "\n",
    "**Note**: You can increase this for better results if you have an M2 Ultra/M3 Max with 64GB+ RAM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45328a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = GRPODataset(\n",
    "    load_dataset(grpo_dataset_name)[\"train\"].take(100),\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    system_key=\"system\",\n",
    "    type_key=\"type\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a753796",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Inspect the Dataset Format\n",
    "\n",
    "Let's examine what the model will see during training. The `train_set.process()` function converts the raw data into tokenized format, and we decode it back to see the actual prompt text.\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "The output should show:\n",
    "- System instructions about using `<think>` and `<answer>` tags\n",
    "- The math problem from the dataset\n",
    "- The chat template used by the model\n",
    "- A generation prompt (where the model will start generating)\n",
    "\n",
    "This is the **exact format** the model will complete during GRPO training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.decode(train_set.process(train_set[0][0]))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e2396",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10: Test Model BEFORE GRPO Training (Baseline)\n",
    "\n",
    "### Why Test Before Training?\n",
    "\n",
    "This generates a **baseline response** so we can compare the model's performance before and after GRPO optimization.\n",
    "\n",
    "### Generation Parameters\n",
    "\n",
    "- `max_tokens=2048`: Up to 2048 tokens for reasoning + answer\n",
    "- `temp=0.6`: Moderate temperature (not too random, not too deterministic)\n",
    "- `top_p=0.95`: Nucleus sampling for diverse responses\n",
    "- `top_k=20`: Consider top 20 tokens at each step\n",
    "- `verbose=True`: Shows generation statistics (tokens/sec, etc.)\n",
    "\n",
    "### What to Expect\n",
    "\n",
    "After cold start training, the model should:\n",
    "- ‚úÖ Use the `<think>` and `<answer>` tags correctly\n",
    "- ‚ö†Ô∏è May not solve the problem correctly yet\n",
    "- ‚ö†Ô∏è Reasoning might be shallow or formulaic\n",
    "\n",
    "**Save this output** - we'll compare it with the post-GRPO output to see the improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_test_output = generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=text,\n",
    "    verbose=True,\n",
    "    max_tokens=zero_max_seq_length//2,\n",
    "    sampler=make_sampler(temp=0.6, top_p=0.95, top_k=20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa61be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 11: Train with GRPO (Group Relative Policy Optimization)\n",
    "\n",
    "### GRPO Training Configuration\n",
    "\n",
    "#### Core GRPO Parameters\n",
    "\n",
    "**Group-Based Optimization:**\n",
    "- `group_size=4`: Generate 4 completions per prompt\n",
    "- GRPO ranks these 4 and learns from relative performance\n",
    "- More efficient than PPO (no value function needed!)\n",
    "\n",
    "**KL Divergence Control:**\n",
    "- `beta=0.1`: Controls KL penalty strength\n",
    "- `epsilon=1e-3`: Lower bound for probability ratio\n",
    "- `epsilon_high=2e-3`: Upper bound for probability ratio\n",
    "- Prevents model from diverging too far from reference\n",
    "\n",
    "#### Generation Parameters\n",
    "\n",
    "- `temperature=0.6`: Sampling temperature\n",
    "- `top_p=0.95`: Nucleus sampling threshold\n",
    "- `top_k=20`: Top-k sampling\n",
    "- `max_completion_length=2048`: Maximum tokens to generate\n",
    "\n",
    "#### Training Hyperparameters\n",
    "\n",
    "- `learning_rate=8e-5`: Moderate learning rate for stable training\n",
    "- `batch_size=1`: Memory efficiency\n",
    "- `gradient_accumulation_steps=1`: With group_size=4, effective batch size is 4\n",
    "- `epochs=1`: One pass through the dataset\n",
    "\n",
    "#### Reward Configuration\n",
    "\n",
    "- `reward_weights=[1.0, 1.0, 1.0, 1.0]`: Equal weighting for all 4 reward functions\n",
    "- You can adjust these to prioritize certain aspects (e.g., [2.0, 1.0, 3.0, 1.0] to emphasize correctness)\n",
    "\n",
    "#### GRPO Variants\n",
    "\n",
    "```python\n",
    "grpo_loss_type=\"grpo\"  # Options:\n",
    "```\n",
    "- **grpo**: Original GRPO algorithm\n",
    "- **bnpo**: Batch Normalized Policy Optimization\n",
    "- **dr_grpo**: Doubly Robust GRPO (experimental)\n",
    "\n",
    "### What Happens During GRPO?\n",
    "\n",
    "1. **Sample**: Generate 4 completions for each prompt\n",
    "2. **Evaluate**: Run all reward functions on completions\n",
    "3. **Rank**: Order completions by total reward\n",
    "4. **Update**: Increase probability of better completions, decrease worse ones\n",
    "5. **Regularize**: Apply KL penalty to prevent over-optimization\n",
    "\n",
    "### Training Time\n",
    "\n",
    "Expect **1-3 hours** depending on your Mac:\n",
    "- M1/M2 base: ~2-3 hours\n",
    "- M1/M2 Pro/Max: ~1-2 hours  \n",
    "- M3/M4 chips: ~1 hour\n",
    "\n",
    "**Pro Tip**: Use `WandBCallback` to track training metrics like:\n",
    "- Average rewards per function\n",
    "- KL divergence\n",
    "- Policy loss\n",
    "- Learning dynamics\n",
    "\n",
    "### Monitoring Progress\n",
    "\n",
    "Watch the console output for:\n",
    "- Loss decreasing over time\n",
    "- Rewards increasing\n",
    "- KL divergence staying stable (not exploding)\n",
    "- Tokens/second (throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d14b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom reward weights if you want to weight them differently\n",
    "custom_reward_weights = [\n",
    "    1.0,  # match_format_exactly\n",
    "    1.0,  # match_format_approximately\n",
    "    1.0,  # check_answer\n",
    "    1.0,  # check_numbers\n",
    "]\n",
    "\n",
    "opt = optim.AdamW(learning_rate=2e-5)\n",
    "\n",
    "args=GRPOTrainingArgs(\n",
    "    batch_size=1,\n",
    "    iters=calculate_iters(train_set, batch_size=1, epochs=1),\n",
    "    val_batches=1,\n",
    "    steps_per_report=5,\n",
    "    steps_per_eval=100,\n",
    "    steps_per_save=200,\n",
    "    adapter_file=adapter_file,\n",
    "    max_seq_length=zero_max_seq_length,\n",
    "    grad_checkpoint=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    beta=0.1,\n",
    "    group_size=4,\n",
    "    epsilon=1e-3,\n",
    "    epsilon_high=2e-3,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    top_k=20,\n",
    "    max_completion_length=zero_max_seq_length//2,\n",
    "    reward_weights=custom_reward_weights,\n",
    "    grpo_loss_type=\"grpo\", # grpo, bnpo, or dr_grpo\n",
    "    # importance_sampling_level=\"sequence\", # token, sequence, None for basic grpo\n",
    ")\n",
    "\n",
    "train_grpo(\n",
    "    model=model,\n",
    "    ref_model=ref_model.freeze(),\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=opt,\n",
    "    train_dataset=CacheDataset(train_set),\n",
    "    args=args,\n",
    "    training_callback=TrainingCallback(),\n",
    "    # training_callback=WandBCallback(\n",
    "    #     project_name=new_model_name,\n",
    "    #     log_dir=adapter_path,\n",
    "    #     config=vars(args),\n",
    "    #     wrapped_callback=TrainingCallback(),\n",
    "    # ),\n",
    "    reward_funcs=[\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    end_answer_token=solution_end\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bc0497",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 12: Test Model AFTER GRPO Training (Compare Results!)\n",
    "\n",
    "### Evaluation Time! üéâ\n",
    "\n",
    "Now we generate with the **GRPO-optimized model** using the exact same:\n",
    "- Prompt (same test problem)\n",
    "- Generation parameters (temp=0.6, top_p=0.95, top_k=20)\n",
    "- Maximum tokens\n",
    "\n",
    "### What Improvements to Expect\n",
    "\n",
    "After GRPO training, the model should show:\n",
    "\n",
    "**‚úÖ Better Structure:**\n",
    "- More consistent use of reasoning tags\n",
    "- Well-organized step-by-step thinking\n",
    "- Clear separation of reasoning and answer\n",
    "\n",
    "**‚úÖ More Accurate Answers:**\n",
    "- Higher correctness rate on math problems\n",
    "- Better numerical accuracy\n",
    "- Fewer hallucinations or wrong calculations\n",
    "\n",
    "**‚úÖ Deeper Reasoning:**\n",
    "- More detailed working out\n",
    "- Explicit intermediate steps\n",
    "- Self-correction and validation\n",
    "\n",
    "### Comparing Before vs After\n",
    "\n",
    "Look at:\n",
    "1. **Format compliance**: Does it follow the structure better?\n",
    "2. **Reasoning quality**: Is the thinking more thorough?\n",
    "3. **Answer correctness**: Is the final answer right?\n",
    "4. **Confidence**: Does the reasoning support the conclusion?\n",
    "\n",
    "The difference can be dramatic! GRPO's reward-based learning often produces much more coherent and accurate reasoning compared to the cold-start model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5cc080",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_test_output = generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=text,\n",
    "    verbose=True,\n",
    "    max_tokens=zero_max_seq_length//2,\n",
    "    sampler=make_sampler(temp=0.6, top_p=0.95, top_k=20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967da9e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 13: Save the Final GRPO-Optimized Model\n",
    "\n",
    "### Merging Adapters with Base Model\n",
    "\n",
    "The `save_pretrained_merged()` function:\n",
    "- Combines the LoRA adapters with the base model weights\n",
    "- Saves a complete, standalone model\n",
    "- Can be loaded like any other MLX model (no special adapter loading needed)\n",
    "\n",
    "### What Gets Saved\n",
    "\n",
    "The saved directory contains:\n",
    "- `model.safetensors` (or split files): Full merged model weights\n",
    "- `config.json`: Model configuration\n",
    "- `tokenizer.json` & `tokenizer_config.json`: Tokenizer files\n",
    "- `chat_template.jinja`: Chat template (if applicable)\n",
    "\n",
    "### Using the Saved Model\n",
    "\n",
    "You can now use this model with:\n",
    "\n",
    "```python\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(new_zero_model_name)\n",
    "response = generate(model, tokenizer, prompt=\"Your problem here\", ...)\n",
    "```\n",
    "\n",
    "No need for adapter loading - it's a complete model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ec908",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pretrained_merged(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    save_path=new_zero_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c6b02",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 14: Push Model to Hugging Face Hub (Optional)\n",
    "\n",
    "### Share Your Model! ü§ó\n",
    "\n",
    "The `push_to_hub()` function uploads your trained model to Hugging Face, making it:\n",
    "- Accessible from anywhere\n",
    "- Easy to share with others\n",
    "- Compatible with the MLX ecosystem\n",
    "\n",
    "### Configuration Parameters\n",
    "\n",
    "- `model_path`: Local path to the model (with or without adapters)\n",
    "- `hf_repo`: Your Hugging Face username/repo name\n",
    "- `api_key`: Your HF token (replace \"HF_KEY\" with actual token or use HF CLI login)\n",
    "- `private=False`: Makes the model public (set to `True` for private repos)\n",
    "- `commit_message`: Description of what you're uploading\n",
    "- `remove_adapters=False`: Keep adapters in the upload (useful for incremental training)\n",
    "\n",
    "### Before Running This Cell\n",
    "\n",
    "1. **Get your HF token**: Go to https://huggingface.co/settings/tokens\n",
    "2. **Replace \"HF_KEY\"** with your actual token, or use:\n",
    "   ```bash\n",
    "   huggingface-cli login\n",
    "   ```\n",
    "3. **Create the repo** on Hugging Face (or set `create_repo=True` if supported)\n",
    "\n",
    "### What Gets Uploaded\n",
    "\n",
    "- All model files\n",
    "- Tokenizer\n",
    "- Configuration\n",
    "- Optional: Adapters (if `remove_adapters=False`)\n",
    "\n",
    "After uploading, others can use your model with:\n",
    "\n",
    "```python\n",
    "model, tokenizer = load(f\"{user_name}/{new_zero_model_name}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully trained a reasoning model using GRPO on Apple Silicon! \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "‚úÖ Two-stage training (Cold Start SFT ‚Üí GRPO) produces better results  \n",
    "‚úÖ Custom reward functions guide specific behaviors  \n",
    "‚úÖ LoRA + quantization enables efficient training on Mac  \n",
    "‚úÖ GRPO optimizes based on relative ranking (no value function needed)  \n",
    "‚úÖ Long context (4K tokens) allows detailed reasoning chains  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Experiment with reward weights**: Adjust `custom_reward_weights` to emphasize different aspects\n",
    "2. **Try different loss types**: Test `bnpo` or `dr_grpo` variants\n",
    "3. **Scale up**: Use more training data if you have sufficient RAM\n",
    "4. **Evaluate systematically**: Test on held-out math problems to measure improvement\n",
    "5. **Apply to other domains**: Adapt reward functions for coding, logical reasoning, etc.\n",
    "\n",
    "### Resources\n",
    "\n",
    "- MLX-LM-LoRA docs: https://github.com/Goekdeniz-Guelmez/mlx-lm-lora\n",
    "- GRPO paper: https://arxiv.org/abs/2402.03300\n",
    "- GSM8K dataset: https://github.com/openai/grade-school-math\n",
    "\n",
    "Happy training! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43001abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "push_to_hub(\n",
    "  model_path=zero_adapter_path,\n",
    "  hf_repo=f\"{user_name}/{new_zero_model_name}\",\n",
    "  api_key=\"HF_KEY\",\n",
    "  private=False,\n",
    "  commit_message=\"Add preference adapters\",\n",
    "  remove_adapters=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-lm-lora-train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
