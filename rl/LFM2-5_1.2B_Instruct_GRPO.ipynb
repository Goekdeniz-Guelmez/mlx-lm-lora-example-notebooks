{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44fd3ce3",
   "metadata": {},
   "source": [
    "# MLX-LM-LoRA GRPO Training Tutorial\n",
    "\n",
    "This notebook demonstrates how to fine-tune language models using **GRPO (Group Relative Policy Optimization)** on Apple Silicon with MLX-LM-LoRA. GRPO is a reinforcement learning technique that trains models to generate outputs that maximize custom reward functions.\n",
    "\n",
    "## What You'll Learn:\n",
    "- Setting up quantized models with LoRA adapters for efficient training\n",
    "- Defining custom reward functions for structured output\n",
    "- Training with GRPO on mathematical reasoning tasks\n",
    "- Evaluating model improvements before and after training\n",
    "- Saving and sharing your trained models\n",
    "\n",
    "## Requirements:\n",
    "- Apple Silicon Mac (M1/M2/M3/M4)\n",
    "- MLX-LM-LoRA library\n",
    "- Sufficient memory for model loading (4-bit quantization helps!)\n",
    "\n",
    "Let's get started! ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa87f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm_lora.utils import save_pretrained_merged, from_pretrained, calculate_iters, push_to_hub\n",
    "from mlx_lm_lora.trainer.grpo_trainer import GRPOTrainingArgs, train_grpo, evaluate_grpo\n",
    "from mlx_lm_lora.trainer.datasets import CacheDataset, GRPODataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from mlx_lm.tuner.utils import print_trainable_parameters\n",
    "from mlx_lm.tuner.callbacks import TrainingCallback, WandBCallback\n",
    "from mlx_lm.generate import generate, make_sampler\n",
    "\n",
    "import mlx.optimizers as optim\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcfcd78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Configure Training Parameters\n",
    "\n",
    "Here we set up all the hyperparameters and configuration needed for GRPO training:\n",
    "\n",
    "### Model Configuration:\n",
    "- **Base Model**: LiquidAI's LFM2.5-1.2B-Instruct - a fast, capable instruction-following model\n",
    "- **Max Sequence Length**: 2048 tokens - balancing memory and context\n",
    "- **LoRA Configuration**: Rank 8, Scale 10.0, targeting 8 layers for efficient fine-tuning\n",
    "- **Quantization**: 4-bit MXFP4 for the trainable model, 6-bit for reference model to save memory\n",
    "\n",
    "### Dataset:\n",
    "- Using GSM8K - a dataset of grade school math problems for reasoning training\n",
    "\n",
    "### Special Tokens:\n",
    "We define custom tokens to structure the model's reasoning output:\n",
    "- `<think>` / `</think>` - For step-by-step reasoning\n",
    "- `<answer>` / `</answer>` - For the final numerical answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"LiquidAI/LFM2.5-1.2B-Instruct\"\n",
    "new_model_name = \"LFM2.5-1.2B-Zero\"\n",
    "user_name = \"Goekdeniz-Guelmez\"\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "adapter_path = f\"./{new_model_name}\"\n",
    "\n",
    "grpo_dataset_name = \"mlx-community/gsm8k\"\n",
    "lora_config = {\n",
    "    \"rank\": 8,\n",
    "    \"dropout\": 0.0,\n",
    "    \"scale\": 10.0,\n",
    "    \"use_dora\": False,\n",
    "    \"num_layers\": 8\n",
    "}\n",
    "quantized_load={\n",
    "    \"bits\": 4,\n",
    "    \"group_size\": 32,\n",
    "    \"mode\": \"mxfp4\"\n",
    "}\n",
    "ref_quantized_load={\n",
    "    \"bits\": 6,\n",
    "    \"group_size\": 128\n",
    "}\n",
    "\n",
    "reasoning_start = \"<think>\"\n",
    "reasoning_end   = \"</think>\"\n",
    "solution_start = \"<answer>\"\n",
    "solution_end = \"</answer>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aaefe0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Load Models and Tokenizer\n",
    "\n",
    "Now we load two models:\n",
    "1. **Training Model**: Quantized with LoRA adapters attached - this is what we'll fine-tune\n",
    "2. **Reference Model**: A frozen copy used for KL divergence calculation in GRPO\n",
    "\n",
    "The reference model prevents the trained model from deviating too far from the original behavior, ensuring stable learning.\n",
    "\n",
    "**Memory Tip**: Using different quantization levels (4-bit vs 6-bit) optimizes the memory footprint for both models on Apple Silicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53528362",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, adapter_file = from_pretrained(\n",
    "    model=model_name,\n",
    "    lora_config=lora_config,\n",
    "    quantized_load=quantized_load,\n",
    "    new_adapter_path=adapter_path\n",
    ")\n",
    "\n",
    "ref_model, _, _ = from_pretrained(\n",
    "    model=model_name,\n",
    "    quantized_load=ref_quantized_load,\n",
    ")\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ab15d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Define Reward Functions\n",
    "\n",
    "Reward functions are the heart of GRPO training! They evaluate how good each generated response is.\n",
    "\n",
    "We define **four reward functions** to train the model on both format correctness and answer accuracy:\n",
    "\n",
    "### 1. `match_format_exactly` (+3.0 points)\n",
    "   - Checks if the output follows the exact expected format with proper reasoning and answer tags\n",
    "\n",
    "### 2. `match_format_approximately` (+0.5 per correct element)\n",
    "   - More lenient scoring for partial format compliance\n",
    "   - Checks for presence of each required tag\n",
    "\n",
    "### 3. `check_answer` (up to +3.0 points)\n",
    "   - Extracts the answer and compares it to the ground truth\n",
    "   - Gives partial credit for close numerical matches\n",
    "   - Penalizes wrong answers\n",
    "\n",
    "### 4. `check_numbers` (+1.5 points)\n",
    "   - Secondary check that looks for any number in the answer section\n",
    "   - Helps reinforce numerical output generation\n",
    "\n",
    "These functions work together to guide the model toward generating well-formatted, accurate mathematical reasoning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eee7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_format = re.compile(\n",
    "    rf\".+?\\n{reasoning_end}\\n{solution_start}(.+?){solution_end}\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "def match_format_exactly(prompts, completions, answer, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def match_format_approximately(prompts, completions, answer, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
    "        score -= 0.5 if response.count(reasoning_start) >= 1 else 0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    responses = [completion[0][\"content\"] if isinstance(completion, list) else completion for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        if guess == true_answer:\n",
    "            score += 3.0\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 1.5\n",
    "        else:\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if   ratio >= 0.9 and ratio <= 1.1: score += 0.5\n",
    "                elif ratio >= 0.8 and ratio <= 1.2: score += 0.25\n",
    "                else: score -= 1.0\n",
    "            except:\n",
    "                score -= 0.5\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "match_numbers = re.compile(\n",
    "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    responses = [completion[0][\"content\"] if isinstance(completion, list) else completion for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            guess       = float(guess.strip())\n",
    "            scores.append(1.5 if guess == true_answer else 0.0)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0473a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Load and Prepare Datasets\n",
    "\n",
    "We create three dataset splits using the `GRPODataset` wrapper:\n",
    "\n",
    "- **Training Set**: 100 examples - for learning\n",
    "- **Validation Set**: 2 examples - for monitoring during training\n",
    "- **Test Set**: 10 examples - for final evaluation\n",
    "\n",
    "The `GRPODataset` handles:\n",
    "- Tokenization of prompts and answers\n",
    "- Proper formatting with system messages\n",
    "- Storage of ground truth answers for reward calculation\n",
    "\n",
    "**Note**: We're using small subsets for this tutorial. For production training, you'd use the full datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45328a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = GRPODataset(\n",
    "    load_dataset(grpo_dataset_name)[\"train\"].take(100),\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    system_key=\"system\",\n",
    "    type_key=\"type\",\n",
    ")\n",
    "test_set = GRPODataset(\n",
    "    load_dataset(grpo_dataset_name)[\"test\"].take(10),\n",
    "    tokenizer,\n",
    "    prompt_key=\"prompt\",\n",
    "    answer_key=\"answer\",\n",
    "    system_key=\"system\",\n",
    "    type_key=\"type\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a753796",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Inspect a Sample Prompt\n",
    "\n",
    "Let's look at what a typical training example looks like after processing. This helps us understand what the model will see as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.decode(test_set.process(test_set[0][0]))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e2396",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Test Model BEFORE Training\n",
    "\n",
    "Before we start training, let's see how the base model performs on a test example. This gives us a baseline to compare against after GRPO training.\n",
    "\n",
    "We use:\n",
    "- **Temperature**: 0.1 (low) for more deterministic output\n",
    "- **Top-p**: 0.95 for nucleus sampling\n",
    "- **Top-k**: 50 for limiting vocabulary choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_test_output = generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=text,\n",
    "    verbose=True,\n",
    "    max_tokens=max_seq_length//2,\n",
    "    sampler=make_sampler(temp=0.1, top_p=0.95, top_k=50)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79069e68",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Evaluate Pre-Training Rewards\n",
    "\n",
    "Now let's run our reward functions on the pre-training output to see the baseline scores:\n",
    "\n",
    "This shows us:\n",
    "- Whether the model already follows the format\n",
    "- How accurate the initial answers are\n",
    "- Which rewards need the most improvement\n",
    "\n",
    "These baseline scores help us track training progress!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answer = \"540\"\n",
    "completions = [before_test_output]\n",
    "\n",
    "print(f\"match_format_exactly: {match_format_exactly([text], completions, [test_answer])[0]}\")\n",
    "print(f\"match_format_approximately: {match_format_approximately([text], completions, [test_answer])[0]}\")\n",
    "print(f\"check_answer: {check_answer([text], completions, [test_answer])[0]}\")\n",
    "print(f\"check_numbers: {check_numbers([text], completions, [test_answer])[0]}\")\n",
    "\n",
    "# Extract the matched number if found\n",
    "generated_match = match_numbers.search(before_test_output)\n",
    "generated_answer = generated_match.group(1) if generated_match else \"None\"\n",
    "print(f\"Answer: {test_answer}, Generated answer: {generated_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa61be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Train with GRPO! ðŸŽ¯\n",
    "\n",
    "Now for the main event - GRPO training!\n",
    "\n",
    "### Key Training Parameters:\n",
    "- **Batch Size**: 1 (conservative for memory)\n",
    "- **Epochs**: 1 full pass through the training data\n",
    "- **Beta**: 0.4 - KL divergence penalty weight\n",
    "- **Group Size**: 4 - number of completions sampled per prompt\n",
    "- **Gradient Checkpointing**: Enabled to save memory\n",
    "- **Loss Type**: Standard GRPO\n",
    "- **Importance Sampling**: Sequence-level for better credit assignment\n",
    "\n",
    "### How GRPO Works:\n",
    "1. For each prompt, the model generates multiple completions (group_size=4)\n",
    "2. Each completion is scored using our reward functions\n",
    "3. The model learns to increase probability of high-reward completions\n",
    "4. The reference model ensures we don't deviate too far from the original behavior\n",
    "\n",
    "### Training Progress:\n",
    "You'll see reports every 5 steps showing:\n",
    "- Average rewards\n",
    "- Training loss\n",
    "- KL divergence from reference model\n",
    "\n",
    "**Note**: Uncomment the WandBCallback section if you want to log training metrics to Weights & Biases!\n",
    "\n",
    "This may take a while depending on your hardware. Grab a coffee! â˜•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d14b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom reward weights if you want to weight them differently\n",
    "custom_reward_weights = [\n",
    "    1.0,  # match_format_exactly\n",
    "    1.0,  # match_format_approximately\n",
    "    1.0,  # check_answer\n",
    "    1.0,  # check_numbers\n",
    "]\n",
    "\n",
    "opt = optim.AdamW(learning_rate=8e-5)\n",
    "\n",
    "args=GRPOTrainingArgs(\n",
    "    batch_size=1,\n",
    "    iters=calculate_iters(train_set, batch_size=1, epochs=1),\n",
    "    val_batches=1,\n",
    "    steps_per_report=5,\n",
    "    steps_per_eval=100,\n",
    "    steps_per_save=200,\n",
    "    adapter_file=adapter_file,\n",
    "    max_seq_length=max_seq_length,\n",
    "    grad_checkpoint=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    beta=0.4,\n",
    "    group_size=4,\n",
    "    epsilon=1e-3,\n",
    "    epsilon_high=2e-3,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    max_completion_length=max_seq_length//2,\n",
    "    reward_weights=custom_reward_weights,\n",
    "    grpo_loss_type=\"grpo\", # grpo, bnpo, or dr_grpo\n",
    "    importance_sampling_level=\"sequence\", # token, sequence, None for basic grpo\n",
    ")\n",
    "\n",
    "train_grpo(\n",
    "    model=model,\n",
    "    ref_model=ref_model.freeze(),\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer=opt,\n",
    "    train_dataset=CacheDataset(train_set),\n",
    "    args=args,\n",
    "    training_callback=TrainingCallback(),\n",
    "    # training_callback=WandBCallback(\n",
    "    #     project_name=new_model_name,\n",
    "    #     log_dir=adapter_path,\n",
    "    #     config=vars(args),\n",
    "    #     wrapped_callback=TrainingCallback(),\n",
    "    # ),\n",
    "    reward_funcs=[\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    end_answer_token=solution_end\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bc0497",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10: Test Model AFTER Training\n",
    "\n",
    "Training is complete! Now let's test the same example again to see how the model has improved.\n",
    "\n",
    "**Note**: We're using slightly different sampling parameters (temp=0.6, top_k=20) to encourage some diversity while still maintaining quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5cc080",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_test_output = generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=text,\n",
    "    verbose=True,\n",
    "    max_tokens=max_seq_length//2,\n",
    "    sampler=make_sampler(temp=0.6, top_p=0.95, top_k=20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abdb694",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 11: Evaluate Post-Training Rewards\n",
    "\n",
    "Let's compare the post-training scores with our baseline!\n",
    "\n",
    "**Look for improvements in:**\n",
    "- Format compliance (higher format scores)\n",
    "- Answer accuracy (higher check_answer scores)\n",
    "- Numerical extraction (check_numbers scores)\n",
    "\n",
    "The difference between pre- and post-training scores shows the effectiveness of GRPO training! ðŸ“ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1907a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answer = \"540\"\n",
    "completions = [after_test_output]\n",
    "\n",
    "print(f\"match_format_exactly: {match_format_exactly([text], completions, [test_answer])[0]}\")\n",
    "print(f\"match_format_approximately: {match_format_approximately([text], completions, [test_answer])[0]}\")\n",
    "print(f\"check_answer: {check_answer([text], completions, [test_answer])[0]}\")\n",
    "print(f\"check_numbers: {check_numbers([text], completions, [test_answer])[0]}\")\n",
    "\n",
    "# Extract the matched number if found\n",
    "generated_match = match_numbers.search(after_test_output)\n",
    "generated_answer = generated_match.group(1) if generated_match else \"None\"\n",
    "print(f\"Answer: {test_answer}, Generated answer: {generated_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967da9e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 12: Full Evaluation on Test Set\n",
    "\n",
    "Time for a comprehensive evaluation! This runs the trained model on the entire test set (10 examples) and computes aggregate statistics.\n",
    "\n",
    "The evaluation provides:\n",
    "- **Average rewards** across all test examples\n",
    "- **Individual reward breakdowns** for each reward function\n",
    "- **Total reward** - the combined score\n",
    "- **GRPO loss metrics** - showing model optimization\n",
    "\n",
    "This gives us a quantitative measure of how well the model performs on unseen data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_grpo(\n",
    "    model=model,\n",
    "    ref_model=ref_model.freeze(),\n",
    "    dataset=CacheDataset(test_set),\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=args.batch_size,\n",
    "    num_batches=None,\n",
    "    beta=args.beta,\n",
    "    epsilon=args.epsilon,\n",
    "    epsilon_high=args.epsilon_high,\n",
    "    group_size=args.group_size,\n",
    "    max_seq_length=max_seq_length,\n",
    "    max_tokens=args.max_completion_length,\n",
    "    temperature=args.temperature,\n",
    "    top_p=args.top_p,\n",
    "    top_k=args.top_k,\n",
    "    min_p=args.min_p,\n",
    "    reward_funcs=[\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    reward_weights=custom_reward_weights,\n",
    "    grpo_loss_type=args.grpo_loss_type,\n",
    "    importance_sampling_level=args.importance_sampling_level,\n",
    "    end_answer_token=solution_end\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c9f96",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 13: Save the Merged Model\n",
    "\n",
    "Now we merge the trained LoRA adapters with the base model and save the full model to disk.\n",
    "\n",
    "This creates a standalone model that includes all the improvements from training, ready to use without needing to load adapters separately.\n",
    "\n",
    "The merged model will be saved in the `LFM2.5-1.2B-Zero` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ec908",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pretrained_merged(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    save_path=new_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c6b02",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 14: Share on Hugging Face Hub ðŸ¤—\n",
    "\n",
    "Finally, let's share our trained model with the community!\n",
    "\n",
    "This uploads the LoRA adapters to Hugging Face Hub, making them accessible to others who want to use or build upon your work.\n",
    "\n",
    "**Before running**: Replace `\"HF_KEY\"` with your actual Hugging Face API token!\n",
    "\n",
    "### What gets uploaded:\n",
    "- LoRA adapter weights\n",
    "- Adapter configuration\n",
    "- (Optional) Full merged model if `remove_adapters=False`\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've successfully:\n",
    "- âœ… Loaded and quantized a model for efficient Apple Silicon training\n",
    "- âœ… Defined custom reward functions for structured output\n",
    "- âœ… Trained a model using GRPO reinforcement learning\n",
    "- âœ… Evaluated improvements in model performance\n",
    "- âœ… Saved and shared your trained model\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different reward functions\n",
    "- Try longer training (more epochs)\n",
    "- Test on different datasets\n",
    "- Adjust hyperparameters like beta, group_size, and temperature\n",
    "- Train larger models with different quantization settings\n",
    "\n",
    "Happy training with MLX-LM-LoRA! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "push_to_hub(\n",
    "  model_path=adapter_path,\n",
    "  hf_repo=f\"{user_name}/{new_model_name}\",\n",
    "  api_key=\"HF_KEY\",\n",
    "  private=False,\n",
    "  commit_message=\"Add preference adapters\",\n",
    "  remove_adapters=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-lm-lora-train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
